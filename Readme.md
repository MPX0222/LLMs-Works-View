# LLMs-Works-View
> ***Focus on Current Works and Surveys in LLMs and its Application in Dialogue Field.***

> ***This responsibility records the survey and study of my road of NLP & LLMs Applications***.

---

### 💬 Brief Introduction of Markdown Files in This Responsibility

* `Readme.md` : Papers and Codes in Google Scholar, Baidu Scholar, Arxiv and Github.
* `Analysis_in_CN_Internet.md` : Analysis, Reviews and Comments in Chinese Internet, including CSDN, ZhiHu, etc.

![Galaxy is our dream](images/Galaxy.png)

---

### 📙 Books
***1. 大规模语言模型：从理论到实践（复旦大学自然语言处理实验室，2023.09.10 v1）*** : 

> | Github Link | Paper Link | Official Website Link | Is Read? |
> |-------------|------------|--------------------|-------------|
> | [Github-Page](https://github.com/intro-llm/intro-llm.github.io) | [PDF-Page](https://intro-llm.github.io/chapter/LLM-TAP.pdf) <br> [BaiDuNetDisk-Page](https://pan.baidu.com/s/1smGQ5ECzDIpvZladuCE59g?pwd=jyz6) | [OfficialWebsite-Page](https://intro-llm.github.io/#chapter) | ✔ |
>
> ***Summarize** : None*

### 🧬 Surveys
***1. A Survey of Large Language Models（Renmin University of China, 2023.03）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|-------------|
> | [Github-Page](https://github.com/rucaibox/llmsurvey) | [Arxiv-Page](https://arxiv.org/abs/2303.18223) | [PaperWithCode-Page](https://paperswithcode.com/paper/a-survey-of-large-language-models) | ✔ |
>
> ***· Tags** : **`Comprehensive Review`** , **`Guidance and Conclusion of Large Language Models`***
>
> ***· Summarize** : A Brief and Comprehensive Introduction of Large Language Models, including Application Fields, Evaluations, Training Methods, Shortage, etc. This thesis elaborated the background and timeline of the LLMs comprehensively. Some Table and Graphs are used to conclude and demonstrate the detail of these Large Language Models. In addition, the thesis explored the training and inference process of the LLMs, showing the relationship and dependency of LLMs and hardwares, such as CPUs and GPUs. Application and Related Method of Large Language Models is expressed in a large chapter, including QA-system, In-context Learning Method, LLMs-based Agents, etc. For example, this paper provide a total introduction of In-Context Learning and Alignment.*


***2. A Survey on LLM-based Autonomous Agents（Renmin University of China）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|-------------|
> | [Github-Page](https://github.com/Paitesanshi/LLM-Agent-Survey) | [Arxiv-Page](https://arxiv.org/pdf/2308.11432.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/a-survey-on-large-language-model-based)  | ✔ |
>
> ***· Tags** : **`Construction and Techniques`** , **`Application`** , **`Evaluation`***
>
> ***· Summarize** : This thesis first discuss the construction of LLM-based autonomous agents, for which they propose a unified framework that encompasses a majority of the previous work. Then, they present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, the authors delve into the evaluation strategies commonly used forLLM-based autonomous agents. Based on the previous studies, they also present several challenges and future directions in this field*

***3. A Survey on Multimodal Large Language Models（University of Science and Technology of China & Tencent YouTu Lab）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|-------------|
> | [Github-Page](https://github.com/bradyfu/awesome-multimodal-large-language-models) | [Arxiv-Page](https://arxiv.org/pdf/2306.13549v1.pdf)  | [PaperWithCode-Page ](https://paperswithcode.com/paper/a-survey-on-multimodal-large-language-models)     |
>
> ***· Tags** :*
>
> ***· Summarize** :*

***4. Recent Advances in Deep Learning Based Dialogue Systems Survey（Nanyang Technological University, 2021.05）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|-------------|
> | NO GITHUB CODE | [Arxiv-Page](https://arxiv.org/pdf/2105.04387.pdf)  |            PaperWithCode-Page        | ✔ |
>
> ***· Tags** :*
>
> ***· Summarize** : A survey about the development of the dialogue system since the application of deep learning methods. This thesis demonstrate the backbone models and methods(CNN, Transformer, Reinforcement Learning), Task-oriented Dialouge system, Open-domain Dialogue system, evaluation methods, datasets. However, since 2022, Large Language Models have take the domainated position of Dialogue System Tasks. So this survey is still limited.*

***5. A Survey on Table Question Answering: Recent Advances（Harbin Institute of Technology (Shenzhen) & Peng Cheng Laboratory）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|-------------|
> | NO GITHUB CODE | [Arxiv-Page](https://arxiv.org/pdf/2207.05270.pdf)  |  [PaperWithCode-Page ](https://paperswithcode.com/paper/a-survey-on-table-question-answering-recent)    |
>
> ***· Tags** :*
>
> ***· Summarize** :*

***6. Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing（Carnegie Mellon University & National University of Singapore, 2023.01）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|-------------|
> | [Github-Page](https://github.com/mingkaid/rl-prompt) | [Arxiv-Page](https://dl.acm.org/doi/pdf/10.1145/3560815) | [PaperWithCode-Page](https://github.com/mingkaid/rl-prompt)  | ✔ |
>
> ***· Tags** : **`Prompt Methods of LLMs`***
>
> ***· Summarize** : A survey of Prompting methods applied in NLP. In this thesis, there are 4 paradigms in NLP: Fully Supervised Learning(Non-NN), Fully Supervised Learning(NN), PreTrain-FineTune(After2018) and PreTrain-Prompt-Predict. With the development of Transformer and Language Models, there are more NLP tasks. If researchers still use PreTrain-FineTune to tune Large Language Model in specific tasks, it would cost massive computing resources, which is extremely unrealistic. Hence, PreTrain-Prompt-Predict is a significant and efficient paradigms that makes LLMs to fit different tasks without tuning of re-training.*


***7. Talking with Machines: A Comprehensive Survey of Emergent Dialogue Systems（University of California, Berkeley）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|-------------|
> | NO GITHUB CODE | [Arxiv-Page](https://arxiv.org/pdf/2305.16324.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/talking-with-machines-a-comprehensive-survey)  |
>
> ***· Tags** :*
>
> ***· Summarize** :*

***8. Reasoning with Language Model Prompting: A Survey（Zhejiang University）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|-------------|
> | [Github-Page](https://github.com/zjunlp/Prompt4ReasoningPapers) | [Arxiv-Page](https://arxiv.org/pdf/2212.09597.pdf) |            PaperWithCode-Page        |
>
> ***· Tags** :*
>
> ***· Summarize** : Survey and Evaluation of QA-System*

***9. A Survey on Evaluation of Large Language Models（Jilin University & Microsoft, 2023.08）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|------------|
> | [Github-Page](https://github.com/mlgroupjlu/llm-eval-survey) | [Arxiv-Page](https://arxiv.org/pdf/2307.03109v7.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/a-survey-on-evaluation-of-large-language) | ✔ |
>
> ***· Tags** : **`Evaluation Methods of LLMs`***
>
> ***· Summarize** : With the development of Large Language models, How to evaluate the performance of LLMs is extremely important to researchers. This thesis conclude the current method of the LLMs evaluation, and disscuss the problems and future trend.*

***10. Evaluating Open-QA Evaluation（Westlake University & Northeastern University, 2023.08）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|------------|
> | [Github-Page](https://github.com/wangcunxiang/qa-eval) | [Arxiv-Page](https://arxiv.org/pdf/2305.12421v3.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/evaluating-open-question-answering-evaluation) | ✔ |
>
> ***· Tags** :*
>
> ***· Summarize** : A Research and Evaluation of QA-Task. This thesis proposed a QA-Evaluation Task and related Dataset EVOUNA.*

***11. A Survey on In-context Learning （Peking University & Shanghai AI Lab, 2023.06）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|------------|
> | [Github-Page](https://github.com/dqxiu/icl_paperlist) | [Arxiv-Page](https://arxiv.org/pdf/2301.00234v3.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/a-survey-for-in-context-learning) | ✔ |
>
> ***· Tags** : **`Important Survey of ICL`***
>
> ***· Summarize** : Survey and Evaluation of ICL. In-Context Learning is firstly proposed in the work of GPT-3. However, it was used as tricks before the conclusion in the paper, and play an indispensable role in the training process of LLMs. ICL uses the demonstration of the tasks or show some samples of these tasks. Then, a designed mode is used to combine these samples to a natural language prompt and add to the input of LLMs. The efficiency and effectiveness of ICL is influenced by the design of samples. In the pre-training phase, the LLM essentially encodes the implicit model through its parameters. In the forward computation phase, LLM can implement learning algorithms such as gradient descent through the examples provided in ICL, or directly compute closed solutions to update these models. In addition Chain-of-Thought(CoT) is proposed to optimize the performance of LLMs in complex inference tasks, which can be combined with ICL in the situation of Few-Shot or Zero-Shot. It's worth noting that In-Context Learning is not as same as the Long Context, which require Large Language Models to store more information from the past interaction.*


***12. The Rise and Potential of Large Language Model Based Agents: A Survey（Fudan University, 2023.09）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|------------|
> | [Github-Page](https://github.com/woooodyy/llm-agent-paper-list) | [Arxiv-Page](https://arxiv.org/pdf/2309.07864v3.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/the-rise-and-potential-of-large-language) | ✔ |
>
> ***· Tags** : **`Agents`***
>
> ***· Summarize** : Survey of LLM-based Agents. The idea and demonstration of the LLM-based Multi-Agent System is extremely inspired. A traditional and excellent work of Multi-Agents is showed in[ 16. Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf](https://paperswithcode.com/paper/deep-model-fusion-a-survey).*

***13. Data-centric Artificial Intelligence: A Survey （Rice University & Texas A&M University, 2023.07）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|------------|
> | [Github-Page](https://github.com/daochenzha/data-centric-ai) | [Arxiv-Page](https://arxiv.org/pdf/2303.10158v3.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/data-centric-artificial-intelligence-a-survey) | ✔ |
>
> ***· Tags** : **`Conclusion of Data Processing Methods and Theories`***
>
> ***· Summarize** : A survey about theories, process flows, augmentations, benchmarks and etc of data. In this paper, the concept and demonstration of Data-centric Artificial Intelligence instead of Data-driven Artificial Intelligence is very inspired. As the saying goes that Model determine the lower limit of the performance and the data determine the upper limit. In my view, the process, methods and tricks of processing original or raw data, or Data Engineering should be given the same status as Model Construction in the Solution.*


***14. Aligning Large Language Models with Human: A Survey （Huawei Noah's Ark Lab, 2023.07）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|------------|
> | [Github-Page](https://github.com/garyyufei/alignllmhumansurvey) | [Arxiv-Page](https://arxiv.org/pdf/2307.12966v1.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/aligning-large-language-models-with-human-a) | ✔ |
>
> ***· Tags** : **`Aligement of LLMs`***
>
> ***· Summarize** : Alignment aims to align the LLM's behavior with human values or preferences. Though LLMs show unparalleled performance in generation, they sometimes generate some words that may mislead users, or is spurious.*


***15. Deep Model Fusion: A Survey（National University of Defense Technology & JD Explore Academy & Beijing Institute of Technology, 2023.09）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|------------|
> | Github-Page | [Arxiv-Page](https://arxiv.org/pdf/2309.15698v1.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/deep-model-fusion-a-survey) | ✔ |
>
> ***· Tags** : **`Fusion`***
>
> ***· Summarize** : Excellient thesis that conclude most deep model fusion methods. The existing methods are summarized into four categories: Mode Connectivity, Alignment, Weight Average, Ensemble Learning, whick contains well-known applications in the past 10 years. Each category is demonstrated by academic terms and formulas. It is the first thesis that the work of deep model fusion in these years are complete summarized. In addition, it is worth noting that the chapter of Alignment is similar to the thesis [14. Aligning Large Language Models with Human: A Survey （Huawei Noah's Ark Lab, 2023.07）](https://arxiv.org/pdf/2307.12966v1.pdf). The former focused on the whole development and most practical techniques of Alignment and the later layed emphasis on the Alignment in Large Language model.*


***16. Explainability for Large Language Models: A Survey （New Jersey Institute of Technology etc, 2023.09）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|------------|
> | Github-Page | [Arxiv-Page](https://arxiv.org/pdf/2309.01029v2.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/explainability-for-large-language-models-a) | ✔ |
>
> ***· Tags** : **`Explainability of LLMs`***
>
> ***· Summarize** : An paper about Explainability for Large Language Models. This thesis demonstrated the explainability in lots of directions, such as gradient explainability, attanion explainability.*


***17. An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning（Westlake University &  WeChat, 2023.08）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|------------|
> | [Github-Page](https://github.com/luoxiaoheics/continual-tune) | [Arxiv-Page](https://browse.arxiv.org/pdf/2308.08747.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/an-empirical-study-of-catastrophic-forgetting) | ✔ |
>
> ***· Tags** : **`Catastrophic Forgetting of LLMs`***
>
> ***· Summarize** : Unsupervised Learning is main training mode of Large Language Models for big training dataset. However, when fine-tuning LLMs for downstream tasks, Catastrophic Forgetting will happen.*

---

### 💊 Excellent Large Language Models

***1. ChatGLM - 6B/130B & ChatGLM 2 - 6B/12B/32B/66B/130B [Based on GLM & GLM-130B]（TsingHua University, 2022.10）***: 

> | Github Link | Paper Link | PaperWithCode Link | Official Website Link | Is Read? |
> |-------------|------------|--------------------|--------------------|------------|
> | [Github(ChatGLM)](https://github.com/THUDM/ChatGLM-6B) <br> [Github(GLM-130B)](https://github.com/thudm/glm-130b) | [Arxiv(GLM-130B)](https://arxiv.org/pdf/2210.02414.pdf) <br> [Arxiv(GLM)](https://arxiv.org/pdf/2103.10360.pdf) | PaperWithCode-Page |[OfficialWeb-Page]( https://chatglm.cn/blog) | GLM-130B ✔ <br> GLM ✔ |
>
> ***· Tags** : **`Pretrained Base Model`** , **`Efficiency`** , **`Low Dependence of Hardwares`***
>
> ***· Summarize** : GLM and GLM-130B faced a lot of challenge when it was firstly proposed, including Lack of CPUs and GPUs, Lack of a Robust Pre-Training Methods, Lack of fast inference solution especially in low computing resources. TsingHua Researchers proposed the training details GLM-130B aiming to provide others a reference about how to train a Large Language Model(100B-Scale), because "It is not easy to train a Large Language Model". GLM-130B use GLM as the backbone, with the methods of Layer Norm, Positional Encoding, FFNs. In the Training Stability Stage, Mixed-Precision(FP16 & FP32) and Embedding Layer Gradient Shrink is used. In addition, GLM-130B was distrubuted in 96 NVIDIA A100-40GB for training(60 Days). In this process, Research teams usually faced the challenges of the lack of GPUs(Even they gained some support from several platforms, they have to edit their codes to fit these platforms or even re-code in C++). Therefore, in the inference stage of GLM-130B, researchers use INT8 and even INT4 to quantize the model weights and the activations, which makes the model can run the inference on RTX 2080Ti.*


***2. LLaMA: Open and Efficient Foundation Language Models  - 7B/13B/33B/65B（FaceBook/Meta AI, 2023.02）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Official Website Link | Is Read? |
> |-------------|------------|--------------------|--------------------|------------|
> | [Github-Page](https://github.com/facebookresearch/llama) | [Arxiv-Page ](https://arxiv.org/pdf/2302.13971v1.pdf)| [PaperWithCode-Page](https://paperswithcode.com/paper/llama-open-and-efficient-foundation-language-1)  | OfficialWeb-Page | ✔ |
>
> ***· Tags** : **`Pretrained Base Model`** , **`LLMs SOTA`***
>
> ***· Summarize** : None*


***3. Baichuan 2 - 7B/13B（Baichuan Intelligent Technology）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Official Website Link | Is Read? |
> |-------------|------------|--------------------|--------------------|------------|
> | [Github-Page](https://github.com/baichuan-inc/Baichuan2) | Arxiv-Page | [PaperWithCode-Page](https://paperswithcode.com/paper/baichuan-2-open-large-scale-language-models)  |[OfficialWeb-Page](https://www.baichuan-ai.com/home) | |
>
> ***· Tags** :*
>
> ***· Summarize** : None*

---

### 💡 Fine-Tuning Methods

***1. LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models （Chinese University of Hong Kong & MIT, 2023.09）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Official Website Link | Is Read? |
> |-------------|------------|--------------------|--------------------|------------|
> | [Github-Page](https://github.com/dvlab-research/longlora) | [Arxiv-Page](https://arxiv.org/pdf/2309.12307v1.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/longlora-efficient-fine-tuning-of-long)  | OfficialWeb-Page | ✔ |
>
> ***· Tags** : **`Fine-tune`** , **`Long-Context`** , , **`SOTA`***
>
> ***· Summarize** : We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shift short attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA demonstrates strong empirical results on various tasks on LLaMA2 models from 7B/13B to 70B. LongLoRA adopts LLaMA2 7B from 4k context to 100k, or LLaMA2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like FlashAttention-2. In addition, to make LongLoRA practical, we collect a dataset, LongQA, for supervised fine-tuning. It contains more than 3k long context question-answer pairs.*

***2. QLoRA: Efficient Finetuning of Quantized LLMs （University of Washington, 2023.05）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Official Website Link | Is Read? |
> |-------------|------------|--------------------|--------------------|------------|
> | [Github-Page](https://github.com/artidoro/qlora) | [Arxiv-Page](https://arxiv.org/pdf/2305.14314v1.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/qlora-efficient-finetuning-of-quantized-llms)  | OfficialWeb-Page | ✔ |
>
> ***· Tags** : **`Fine-tune`** , **`Long-Context`** , , **`Efficiency`***
>
> ***· Summarize** : 

---

### ⏳ Development and Extension of Large Language Models

***1. XInsight: eXplainable Data Analysis Through The Lens of Causality（Hong Kong University of Science and Technology, 2023.07）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Official Website Link | Is Read? |
> |-------------|------------|--------------------|--------------------|------------|
> | [Github-Page]() | [Arxiv-Page](https://arxiv.org/pdf/2207.12718v4.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/xinsight-explainable-data-analysis-through)  | OfficialWeb-Page | |
>
> ***· Tags** : **`Causality`***
>
> ***· Summarize** :*


---

### 🗃 Works of Large Language Model Based Agents

***1. Agents: An Open-source Framework for Autonomous Language Agents（AIWaves Inc. & Zhejiang University, 2023.09）*** : 


> | Github Link | Paper Link | PaperWithCode Link | Official Website Link | Is Read? |
> |-------------|------------|--------------------|--------------------|------------|
> | [Github-Page](https://github.com/aiwaves-cn/agents ) | [Arxiv-Page](https://arxiv.org/pdf/2309.07870.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/agents-an-open-source-framework-for) |[OfficialWeb-Page](http://www.aiwaves-agents.com/) | |
>
> ***· Tags** :*
>
> ***· Summarize** : None*


***2. Generative Agents: Interactive Simulacra of Human Behavior（Stanford University, 2023.04）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Official Website Link | Is Read? |
> |-------------|------------|--------------------|--------------------|------------|
> | [Github-Page](https://github.com/joonspk-research/generative_agents) | [Arxiv-Page](https://arxiv.org/pdf/2304.03442v2.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/generative-agents-interactive-simulacra-of)  | OfficialWeb-Page | ✔ |
>
> ***· Tags** : **`Multi-Agents`** , **`AI-Town`** , **`Techniques`***
>
> ***· Summarize** : Multi-Agents is an attractive research direction now. This project construct a Agents Society that includes 25 LLM-based Agents. Researchers provides the design methods of this AI-Agents Towns and technique details, evaluation methods. This thesis is extremely important for LLM-based Multi-Agents Projects.*


***3. Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf（Tsinghua University, 2023.09）*** : 

> | Github Link | Paper Link | PaperWithCode Link | Is Read? |
> |-------------|------------|--------------------|------------|
> | Github-Page | [Arxiv-Page](https://arxiv.org/pdf/2309.04658v1.pdf) | [PaperWithCode-Page](https://paperswithcode.com/paper/exploring-large-language-models-for) |  |
>
> ***· Tags** :*
>
> ***· Summarize** : An paper about Multi-Agents.*




